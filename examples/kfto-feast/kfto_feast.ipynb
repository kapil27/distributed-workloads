{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c6b88d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. Setup Feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f55ab-75fe-48a8-9ae7-58ee5f1a98dc",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install feast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f213e5b8-89b6-4bf2-b805-c5fe22300528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feast SDK Version: \"0.48.1\"\n"
     ]
    }
   ],
   "source": [
    "!feast version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e9a77-91c8-45cd-9035-b2af56519045",
   "metadata": {},
   "source": [
    "### 2. Create a feature repository : Initiate a Feast project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6338b4-008e-41d4-99b1-e25fab5938aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating a new Feast repository in \u001b[1m\u001b[32m/opt/app-root/src/myproject\u001b[0m.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!feast init myproject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba2270-f865-4fdf-8f5b-4dd63714c093",
   "metadata": {},
   "source": [
    "### 3. Inspecting the feature repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5c9a52-2cee-4e8b-8887-4dbef401147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c4d02d1-f359-4d6f-8d07-c730e8a6f4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/myproject/feature_repo\n"
     ]
    }
   ],
   "source": [
    "%cd myproject/feature_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9153681d-2813-4e23-910c-a91973fcc916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      " +-- __init__.py\n",
      " +-- example_repo.py\n",
      " +-- test_workflow.py\n",
      " +-- data\n",
      " +--  |-- driver_stats.parquet\n",
      " +-- feature_store.yaml\n",
      " +-- __pycache__\n",
      " +--  |-- __init__.cpython-311.pyc\n",
      " +--  |-- test_workflow.cpython-311.pyc\n",
      " +--  |-- example_repo.cpython-311.pyc\n"
     ]
    }
   ],
   "source": [
    "# Inspect the feast repo path files. Displaying folder strucuture as tree.\n",
    "!find . | sed -e 's/[^-][^\\/]*\\// |-- /g' -e 's/|-- \\(.*\\)/+-- \\1/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea9c1de-7242-422a-aca1-f7c570fda915",
   "metadata": {},
   "source": [
    "### Key files : \n",
    "* `example_repo.py` file will have the code to create feast objects such as FeatureView, FeatureServices and OnDemandFeatureViews required to demonstrate this example. -- _myproject/feature_repo/example_repo.py_\n",
    "\n",
    "* `feature_store.yaml` file will have all the configurations related to feast. -- _my_feast_project/feature_repo/feature_store.yaml_\n",
    "\n",
    "* `test_workflow.py` contains the python code to demonstrate runining all key Feast commands, including defining, retrieving, and pushing features.  -- _my_feast_project/feature_repo/test_workflow.py_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a103bba-966f-4993-8699-0a8e6ebe48eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project: myproject\n",
      "# By default, the registry is a file (but can be turned into a more scalable SQL-backed registry)\n",
      "registry: data/registry.db\n",
      "# The provider primarily specifies default offline / online stores & storing the registry in a given cloud\n",
      "provider: local\n",
      "online_store:\n",
      "    type: sqlite\n",
      "    path: data/online_store.db\n",
      "entity_key_serialization_version: 2\n",
      "# By default, no_auth for authentication and authorization, other possible values kubernetes and oidc. Refer the documentation for more details.\n",
      "auth:\n",
      "    type: no_auth\n"
     ]
    }
   ],
   "source": [
    "!cat feature_store.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a7124-3513-4683-9fc0-12384d59066a",
   "metadata": {},
   "source": [
    "> [!NOTE] :\n",
    "> \n",
    "> File `data/driver_stats.parquet` is generated by the _feast init_ command and it acts as an historical information source to this example. We have defined this source in the `myproject/feature_repo/example_repo.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92623ea5-c091-4df9-9a2b-9c7f877120d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>driver_id</th>\n",
       "      <th>conv_rate</th>\n",
       "      <th>acc_rate</th>\n",
       "      <th>avg_daily_trips</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-26 13:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.004254</td>\n",
       "      <td>0.155951</td>\n",
       "      <td>370</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-26 14:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>0.145077</td>\n",
       "      <td>48</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-26 15:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.429421</td>\n",
       "      <td>0.184800</td>\n",
       "      <td>183</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-26 16:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.653685</td>\n",
       "      <td>0.019408</td>\n",
       "      <td>481</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-26 17:00:00+00:00</td>\n",
       "      <td>1005</td>\n",
       "      <td>0.800338</td>\n",
       "      <td>0.426761</td>\n",
       "      <td>105</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>2025-04-10 11:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.217492</td>\n",
       "      <td>0.468898</td>\n",
       "      <td>496</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>2025-04-10 12:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.914503</td>\n",
       "      <td>0.137143</td>\n",
       "      <td>855</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>2021-04-12 07:00:00+00:00</td>\n",
       "      <td>1001</td>\n",
       "      <td>0.668744</td>\n",
       "      <td>0.717003</td>\n",
       "      <td>134</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>2025-04-03 01:00:00+00:00</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.402941</td>\n",
       "      <td>0.269871</td>\n",
       "      <td>430</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>2025-04-03 01:00:00+00:00</td>\n",
       "      <td>1003</td>\n",
       "      <td>0.402941</td>\n",
       "      <td>0.269871</td>\n",
       "      <td>430</td>\n",
       "      <td>2025-04-10 13:50:06.169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1807 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               event_timestamp  driver_id  conv_rate  acc_rate  \\\n",
       "0    2025-03-26 13:00:00+00:00       1005   0.004254  0.155951   \n",
       "1    2025-03-26 14:00:00+00:00       1005   0.356659  0.145077   \n",
       "2    2025-03-26 15:00:00+00:00       1005   0.429421  0.184800   \n",
       "3    2025-03-26 16:00:00+00:00       1005   0.653685  0.019408   \n",
       "4    2025-03-26 17:00:00+00:00       1005   0.800338  0.426761   \n",
       "...                        ...        ...        ...       ...   \n",
       "1802 2025-04-10 11:00:00+00:00       1001   0.217492  0.468898   \n",
       "1803 2025-04-10 12:00:00+00:00       1001   0.914503  0.137143   \n",
       "1804 2021-04-12 07:00:00+00:00       1001   0.668744  0.717003   \n",
       "1805 2025-04-03 01:00:00+00:00       1003   0.402941  0.269871   \n",
       "1806 2025-04-03 01:00:00+00:00       1003   0.402941  0.269871   \n",
       "\n",
       "      avg_daily_trips                 created  \n",
       "0                 370 2025-04-10 13:50:06.169  \n",
       "1                  48 2025-04-10 13:50:06.169  \n",
       "2                 183 2025-04-10 13:50:06.169  \n",
       "3                 481 2025-04-10 13:50:06.169  \n",
       "4                 105 2025-04-10 13:50:06.169  \n",
       "...               ...                     ...  \n",
       "1802              496 2025-04-10 13:50:06.169  \n",
       "1803              855 2025-04-10 13:50:06.169  \n",
       "1804              134 2025-04-10 13:50:06.169  \n",
       "1805              430 2025-04-10 13:50:06.169  \n",
       "1806              430 2025-04-10 13:50:06.169  \n",
       "\n",
       "[1807 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect driver_stats data\n",
    "import pandas as pd\n",
    "pd.read_parquet(\"data/driver_stats.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac0167-a4ae-4edb-8177-58dc179caf19",
   "metadata": {},
   "source": [
    "### 4. Creating Feast objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fadbdaab-bea0-49a2-8937-e2e30d733755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"vector_enabled\" in \"SqliteOnlineStoreConfig\" shadows an attribute in parent \"VectorStoreConfig\"\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"vector_len\" in \"SqliteOnlineStoreConfig\" shadows an attribute in parent \"VectorStoreConfig\"\n",
      "  warnings.warn(\n",
      "/opt/app-root/src/myproject/feature_repo/example_repo.py:27: DeprecationWarning: Entity value_type will be mandatory in the next release. Please specify a value_type for entity 'driver'.\n",
      "  driver = Entity(name=\"driver\", join_keys=[\"driver_id\"])\n",
      "Applying changes for project myproject\n",
      "/opt/app-root/lib64/python3.11/site-packages/feast/feature_store.py:581: RuntimeWarning: On demand feature view is an experimental feature. This API is stable, but the functionality does not scale well for offline retrieval\n",
      "  warnings.warn(\n",
      "Created project \u001b[1m\u001b[32mmyproject\u001b[0m\n",
      "Created entity \u001b[1m\u001b[32mdriver\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_hourly_stats_fresh\u001b[0m\n",
      "Created feature view \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "Created on demand feature view \u001b[1m\u001b[32mtransformed_conv_rate\u001b[0m\n",
      "Created on demand feature view \u001b[1m\u001b[32mtransformed_conv_rate_fresh\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mdriver_activity_v3\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mdriver_activity_v1\u001b[0m\n",
      "Created feature service \u001b[1m\u001b[32mdriver_activity_v2\u001b[0m\n",
      "\n",
      "Created sqlite table \u001b[1m\u001b[32mmyproject_driver_hourly_stats_fresh\u001b[0m\n",
      "Created sqlite table \u001b[1m\u001b[32mmyproject_driver_hourly_stats\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You have not yet created any feast objects. In order to do that you have to execute the `feast apply` command on the directory where feature_store.yaml exists.\n",
    "# this command will actual creates the feast objects mentioned in `example_repo.py`\n",
    "!feast apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64675e-0eed-4117-9a81-2dc66d2899f4",
   "metadata": {},
   "source": [
    "### 5. Retrieving historical features (for training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b665b5e-c40c-43bc-92f9-947e22652cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Feature schema -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 10 columns):\n",
      " #   Column                              Non-Null Count  Dtype              \n",
      "---  ------                              --------------  -----              \n",
      " 0   driver_id                           5 non-null      int64              \n",
      " 1   event_timestamp                     5 non-null      datetime64[ns, UTC]\n",
      " 2   label_driver_reported_satisfaction  5 non-null      int64              \n",
      " 3   val_to_add                          5 non-null      int64              \n",
      " 4   val_to_add_2                        5 non-null      int64              \n",
      " 5   conv_rate                           5 non-null      float32            \n",
      " 6   acc_rate                            5 non-null      float32            \n",
      " 7   avg_daily_trips                     5 non-null      int32              \n",
      " 8   conv_rate_plus_val1                 5 non-null      float64            \n",
      " 9   conv_rate_plus_val2                 5 non-null      float64            \n",
      "dtypes: datetime64[ns, UTC](1), float32(2), float64(2), int32(1), int64(4)\n",
      "memory usage: 472.0 bytes\n",
      "None\n",
      "\n",
      "----- Example features -----\n",
      "\n",
      "   driver_id           event_timestamp  label_driver_reported_satisfaction  \\\n",
      "0       1001 2021-04-12 10:59:42+00:00                                   1   \n",
      "1       1002 2021-04-12 08:12:10+00:00                                   5   \n",
      "2       1003 2021-04-12 16:40:26+00:00                                   3   \n",
      "3       1004 2021-04-12 12:30:00+00:00                                   4   \n",
      "4       1005 2021-04-12 14:15:30+00:00                                   2   \n",
      "\n",
      "   val_to_add  val_to_add_2  conv_rate  acc_rate  avg_daily_trips  \\\n",
      "0           1            10   0.668744  0.717003              134   \n",
      "1           2            20   0.112177  0.605126              321   \n",
      "2           3            30   0.850690  0.662039              618   \n",
      "3           4            40   0.215856  0.717347              566   \n",
      "4           5            50   0.486344  0.261190              710   \n",
      "\n",
      "   conv_rate_plus_val1  conv_rate_plus_val2  \n",
      "0             1.668744            10.668744  \n",
      "1             2.112177            20.112177  \n",
      "2             3.850690            30.850690  \n",
      "3             4.215856            40.215856  \n",
      "4             5.486344            50.486344  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"vector_enabled\" in \"SqliteOnlineStoreConfig\" shadows an attribute in parent \"VectorStoreConfig\"\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.11/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"vector_len\" in \"SqliteOnlineStoreConfig\" shadows an attribute in parent \"VectorStoreConfig\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "from feast import FeatureStore\n",
    "\n",
    "# Initialize Feast feature store\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "# Define entity to retrieve data for\n",
    "timestamp_now = pd.to_datetime(\"now\", utc=True)\n",
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        # entity's join key -> entity values\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1005],\n",
    "        # \"event_timestamp\" (reserved key) -> timestamps\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 12, 30, 0),\n",
    "            datetime(2021, 4, 12, 14, 15, 30)\n",
    "        ],\n",
    "        # (optional) label name -> label values. Feast does not process these\n",
    "        \"label_driver_reported_satisfaction\": [1, 5, 3, 4, 2],\n",
    "        # values we're using for an on-demand transformation\n",
    "        \"val_to_add\": [1, 2, 3, 4, 5],\n",
    "        \"val_to_add_2\": [10, 20, 30, 40, 50],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retrieve historical features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_hourly_stats:avg_daily_trips\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "        \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "    ],\n",
    ").to_df()\n",
    "\n",
    "print(\"----- Feature schema -----\\n\")\n",
    "print(training_df.info())\n",
    "\n",
    "print()\n",
    "print(\"----- Example features -----\\n\")\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058dde6-edfc-4eb0-a6ac-5399dcdd7bee",
   "metadata": {},
   "source": [
    "### 6. Data Preprocessing: Converting Driver Statistics to JSONL Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0584696a-9345-4297-a824-f9bfbb53136c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Driver stats document objects converted and saved as JSONL to 'driver_stats_documents.jsonl'.\n"
     ]
    }
   ],
   "source": [
    "## Transforming Driver Stats DataFrame to JSONL Document Objects for LLM Training\n",
    "\n",
    "import json\n",
    "\n",
    "# Assuming training_df is already defined and populated, store all document objects in an array\n",
    "documents = []\n",
    "\n",
    "for i in range(len(training_df)):\n",
    "    doc = {\n",
    "        \"driver_id\": int(training_df['driver_id'][i]),\n",
    "        \"conv_rate\": float(training_df['conv_rate'][i]),\n",
    "        \"acc_rate\": float(training_df['acc_rate'][i]),\n",
    "        \"avg_daily_trips\": int(training_df['avg_daily_trips'][i]),\n",
    "    }\n",
    "    documents.append(doc)\n",
    "\n",
    "# Save the transformed data in a JSONL format for LLM training\n",
    "stats_document_jsonl_file=\"driver_stats_documents.jsonl\"\n",
    "with open(stats_document_jsonl_file, \"w\") as f:\n",
    "    for doc in documents:\n",
    "        f.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "print(f\">> Driver stats document objects converted and saved as JSONL to '{stats_document_jsonl_file}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "125922e5-8b07-4bea-beb9-28a610255565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training dataset saved to 'driver_stats_training.jsonl'\n"
     ]
    }
   ],
   "source": [
    "##Creating Training Examples: Generating Driver Performance Summaries from JSONL Data\n",
    "\n",
    "import json\n",
    "\n",
    "# Define file paths\n",
    "input_file = \"driver_stats_documents.jsonl\"   # Input file with driver stats\n",
    "output_file = \"driver_stats_training.jsonl\"  # Output file for training data\n",
    "\n",
    "instruction_text = \"Summarize the driver's performance metrics.\"\n",
    "\n",
    "# Function to generate a summary for the driver's performance\n",
    "def generate_summary(record):\n",
    "    summary = (\n",
    "        f\"Driver {record['driver_id']} has a conversion rate of {record['conv_rate']:.2%}, \"\n",
    "        f\"an acceleration rate of {record['acc_rate']:.2%}, and completes an average of {record['avg_daily_trips']} daily trips.\"\n",
    "    )\n",
    "    return summary\n",
    "\n",
    "# Process the input file and create training examples\n",
    "with open(input_file, \"r\") as fin, open(output_file, \"w\") as fout:\n",
    "    for line in fin:\n",
    "        record = json.loads(line.strip())\n",
    "        input_text = (\n",
    "            f\"Driver ID: {record['driver_id']}, \"\n",
    "            f\"Conversion Rate: {record['conv_rate']:.4f}, \"\n",
    "            f\"Acceleration Rate: {record['acc_rate']:.4f}, \"\n",
    "            f\"Average Daily Trips: {record['avg_daily_trips']}\"\n",
    "        )\n",
    "        output_text = generate_summary(record)\n",
    "        example = {\n",
    "            \"instruction\": instruction_text,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "        fout.write(json.dumps(example) + \"\\n\")\n",
    "\n",
    "print(f\">> Training dataset saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9063e0-870e-423b-8c0a-fa8ccc975a64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dfc73e3-3c41-4953-b191-655cfd832d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## sample dataset loading\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"driver_stats_training.jsonl\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e5c06-d00c-4e08-afea-7e598980e87e",
   "metadata": {},
   "source": [
    "### 7. Configuring Tokenization for Fine-Tuning: Mapping Instructions and Outputs with Granite Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21e3ce-4961-4d0b-bcb3-6d5e8fa770e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2901138c821145d79f3448c9036fb417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689e311a00884ddba64cbc54eb40b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4be7b6f8ba491f996e55b9c4e80e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47b12066b444bb786ddbb3c783fe391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "## Sample preprocessing and Tokenization of Instruction-Based Dataset Using Hugging Face Transformers\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = \"ibm-granite/granite-3.0-1b-a400m-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    input_text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\"\n",
    "    target_text = example['output']\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(target_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=False)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0137410-8876-4722-871e-0ef5310eb9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U kubeflow-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f2ec2-fc49-44d1-a763-db59267491b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kubeflow-training\n",
      "Version: 1.9.1\n",
      "Summary: Training Operator Python SDK\n",
      "Home-page: https://github.com/kubeflow/training-operator/tree/master/sdk/python\n",
      "Author: Kubeflow Authors\n",
      "Author-email: hejinchi@cn.ibm.com\n",
      "License: Apache License Version 2.0\n",
      "Location: /opt/app-root/lib64/python3.11/site-packages\n",
      "Requires: certifi, kubernetes, retrying, setuptools, six, urllib3\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# the kubeflow-training version must be >= `1.9.0`, as the `env_vars` support was first introduced in this version - https://github.com/kubeflow/trainer/releases/tag/v1.9.0\n",
    "%pip show kubeflow-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cda1e8-0457-40e5-aaa3-8c412763ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "num_gpus = \"1\" # Number of GPUs per worker node\n",
    "openshift_api_url = \"has to be specified\"\n",
    "namespace = \"feast-kfto-finetuning\" # Update this to match the name of your data science project\n",
    "token = \"has to be specified\"\n",
    "training_image= \"quay.io/modh/training:py311-cuda121-torch241\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d54f1b2-51fa-4525-809a-e7d1e59438d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigMap 'training-config' created successfully in namespace 'feast-kfto-finetuning'.\n"
     ]
    }
   ],
   "source": [
    "# create configmap to store driver_stats_training.jsonl files\n",
    "import os\n",
    "from kubernetes import client, config\n",
    "from kubernetes.client.rest import ApiException\n",
    "\n",
    "# Define ConfigMap name\n",
    "CONFIGMAP_NAME = \"training-config\"\n",
    "\n",
    "# Define headers for API authentication\n",
    "configuration = client.Configuration()\n",
    "configuration.host = openshift_api_url\n",
    "configuration.verify_ssl = False  # Set to True if using a valid CA certificate\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# Load Kubernetes client configuration\n",
    "client.Configuration.set_default(configuration)\n",
    "api_instance = client.CoreV1Api()\n",
    "\n",
    "# Read training dataset file from the current directory\n",
    "DATASET_FILE = \"driver_stats_training.jsonl\"\n",
    "\n",
    "if os.path.exists(DATASET_FILE):\n",
    "    with open(DATASET_FILE, \"r\", encoding=\"utf-8\") as file:\n",
    "        dataset_content = file.read()\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Dataset file '{DATASET_FILE}' not found in current directory.\")\n",
    "\n",
    "# Define ConfigMap data\n",
    "if dataset_content:\n",
    "    configmap_data = {\n",
    "        f\"{DATASET_FILE}\": dataset_content\n",
    "    }\n",
    "\n",
    "    # Create the ConfigMap object\n",
    "    configmap = client.V1ConfigMap(\n",
    "        metadata=client.V1ObjectMeta(name=CONFIGMAP_NAME),\n",
    "        data=configmap_data\n",
    "    )\n",
    "    \n",
    "    # Apply ConfigMap to Kubernetes\n",
    "    try:\n",
    "        api_instance.create_namespaced_config_map(namespace=namespace, body=configmap)\n",
    "        print(f\"ConfigMap '{CONFIGMAP_NAME}' created successfully in namespace '{namespace}'.\")\n",
    "    except ApiException as e:\n",
    "        if e.status == 409:\n",
    "            print(f\"ConfigMap '{CONFIGMAP_NAME}' already exists. Updating it...\")\n",
    "            api_instance.replace_namespaced_config_map(name=CONFIGMAP_NAME, namespace=namespace, body=configmap)\n",
    "        else:\n",
    "            print(f\"Error creating ConfigMap: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a0dab",
   "metadata": {},
   "source": [
    "### 8. Define a training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c79cb-a5c5-453a-a375-5e246623d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_upload():\n",
    "    import os\n",
    "    import json\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup, logging\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    from torch.optim import AdamW\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "    logging.set_verbosity_debug()\n",
    "\n",
    "    # Helper function to log both global and local rank info\n",
    "    def log_rank_info(stage: str):\n",
    "        global_rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "        device = torch.device(\"cuda\", local_rank)\n",
    "        try:\n",
    "            device_name = torch.cuda.get_device_name(local_rank)\n",
    "        except Exception as e:\n",
    "            device_name = f\"Unknown (error: {e})\"\n",
    "        print(f\"[{stage} | Global Rank {global_rank} | Local Rank {local_rank}]: Using device: {device} ({device_name})\")\n",
    "\n",
    "    # Read configuration flags from environment variables\n",
    "    use_lora = os.getenv(\"USE_LORA\", \"false\").lower() in [\"true\", \"1\"]\n",
    "    use_qlora = os.getenv(\"USE_QLORA\", \"false\").lower() in [\"true\", \"1\"]\n",
    "    use_deepspeed = os.getenv(\"USE_DEEPSPEED\", \"false\").lower() in [\"true\", \"1\"]\n",
    "\n",
    "    # For QLoRA, we require DeepSpeed for optimal performance. Switch on DeepSpeed if needed.\n",
    "    if use_qlora and not use_deepspeed:\n",
    "        print(\"QLoRA typically requires DeepSpeed for optimal performance. Enabling DeepSpeed.\")\n",
    "        use_deepspeed = True\n",
    "\n",
    "    if use_lora or use_qlora:\n",
    "        from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "    if use_deepspeed:\n",
    "        import deepspeed\n",
    "\n",
    "    # Set CUDA memory configuration to help mitigate fragmentation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    # Initialize distributed training (NCCL backend)\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    log_rank_info(\"After Device Setup\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    model_name = os.environ.get(\"MODEL_NAME\", \"ibm-granite/granite-3.0-1b-a400m-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load model â€“ if using QLoRA, load in 4-bit mode; otherwise, load normally.\n",
    "    if use_qlora:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        # When using device_map=\"auto\", the model is automatically split across GPUs.\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        log_rank_info(\"After Loading Model in 4-bit Mode (QLoRA)\")\n",
    "    else:\n",
    "        # Standard loading onto the designated GPU.\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name).to(torch.device(\"cuda\", local_rank))\n",
    "        log_rank_info(\"After Standard Model Loading\")\n",
    "\n",
    "    # Apply LoRA or QLoRA if enabled.\n",
    "    if use_lora or use_qlora:\n",
    "        # Common LoRA configuration\n",
    "        lora_r = int(os.environ.get(\"LORA_R\", \"8\"))\n",
    "        lora_alpha = int(os.environ.get(\"LORA_ALPHA\", \"32\"))\n",
    "        lora_dropout = float(os.environ.get(\"LORA_DROPOUT\", \"0.1\"))\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        # For QLoRA, prepare the model for k-bit training.\n",
    "        if use_qlora:\n",
    "            from peft import prepare_model_for_kbit_training\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        log_rank_info(\"After Applying LoRA/QLoRA\")\n",
    "\n",
    "    # Custom JSONL dataset\n",
    "    class JsonlDataset(Dataset):\n",
    "        def __init__(self, file_path, tokenizer, max_length=512):\n",
    "            self.samples = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    record = json.loads(line)\n",
    "                    self.samples.append(record)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            record = self.samples[idx]\n",
    "            input_text = record.get(\"input\", \"\")\n",
    "            target_text = record.get(\"output\", \"\")\n",
    "            # Combine input and target using the EOS token (or newline) as separator\n",
    "            separator = self.tokenizer.eos_token if self.tokenizer.eos_token is not None else \"\\n\"\n",
    "            combined_text = input_text + separator + target_text\n",
    "            encoded = self.tokenizer(\n",
    "                combined_text, truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": encoded.input_ids.squeeze(0),\n",
    "                \"attention_mask\": encoded.attention_mask.squeeze(0),\n",
    "                \"labels\": encoded.input_ids.squeeze(0)\n",
    "            }\n",
    "\n",
    "    # Collate function for DataLoader\n",
    "    def collate_fn(batch):\n",
    "        input_ids = pad_sequence([x[\"input_ids\"] for x in batch], batch_first=True, padding_value=0)\n",
    "        attention_mask = pad_sequence([x[\"attention_mask\"] for x in batch], batch_first=True, padding_value=0)\n",
    "        labels = pad_sequence([x[\"labels\"] for x in batch], batch_first=True, padding_value=-100)\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "    # Ensure S3 bucket exists; if not, attempt to create it.\n",
    "    def ensure_bucket_exists(s3_client, bucket, region):\n",
    "        try:\n",
    "            s3_client.head_bucket(Bucket=bucket)\n",
    "            print(f\"Bucket {bucket} exists.\")\n",
    "        except ClientError as e:\n",
    "            error_code = int(e.response['Error']['Code'])\n",
    "            if error_code == 404:\n",
    "                print(f\"Bucket {bucket} does not exist. Creating it...\")\n",
    "                try:\n",
    "                    if region == \"us-east-1\":\n",
    "                        s3_client.create_bucket(Bucket=bucket)\n",
    "                    else:\n",
    "                        s3_client.create_bucket(\n",
    "                            Bucket=bucket,\n",
    "                            CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                        )\n",
    "                    print(f\"Bucket {bucket} created.\")\n",
    "                except ClientError as create_err:\n",
    "                    print(f\"Failed to create bucket {bucket}: {create_err}\")\n",
    "                    raise create_err\n",
    "            else:\n",
    "                print(f\"Error checking bucket: {e}\")\n",
    "                raise e\n",
    "\n",
    "    # Upload a directory recursively to S3\n",
    "    def upload_directory_to_s3(directory, bucket, s3_prefix, s3_client):\n",
    "        for root, _, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                local_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(local_path, directory)\n",
    "                s3_key = os.path.join(s3_prefix, relative_path) if s3_prefix else relative_path\n",
    "                try:\n",
    "                    s3_client.upload_file(local_path, bucket, s3_key)\n",
    "                    print(f\"Uploaded {local_path} to s3://{bucket}/{s3_key}\")\n",
    "                except ClientError as e:\n",
    "                    print(f\"Failed to upload {local_path} to S3: {e}\")\n",
    "\n",
    "    # Prepare dataset and DataLoader\n",
    "    dataset_path = os.environ.get(\"DATA_PATH\", \"/mnt/config/driver_stats_training.jsonl\")\n",
    "    output_dir = os.environ.get(\"OUTPUT_DIR\", \"fine-tuned-model\")\n",
    "    dataset = JsonlDataset(file_path=dataset_path, tokenizer=tokenizer)\n",
    "    sampler = DistributedSampler(dataset)\n",
    "    batch_size = int(os.environ.get(\"BATCH_SIZE\", \"2\"))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn)\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    learning_rate = float(os.environ.get(\"LEARNING_RATE\", \"5e-5\"))\n",
    "    num_epochs = int(os.environ.get(\"NUM_EPOCHS\", \"3\"))\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    num_training_steps = len(dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "\n",
    "    # If using DeepSpeed, initialize the DeepSpeed engine\n",
    "    if use_deepspeed:\n",
    "        ds_config = {\n",
    "            \"train_micro_batch_size_per_gpu\": batch_size,\n",
    "            \"gradient_accumulation_steps\": 1,\n",
    "            \"fp16\": {\"enabled\": True},\n",
    "            \"zero_optimization\": {\"stage\": 2},\n",
    "        }\n",
    "        model, optimizer, _, scheduler = deepspeed.initialize(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=None,\n",
    "            config=ds_config\n",
    "        )\n",
    "        log_rank_info(\"After DeepSpeed Initialization\")\n",
    "    else:\n",
    "        # If not using DeepSpeed and not QLoRA (which is unlikely), you may wrap with FSDP.\n",
    "        from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, CPUOffload\n",
    "\n",
    "        def lora_auto_wrap_policy(module, recurse, nonwrapped_numel=0):\n",
    "            return any(p.requires_grad for p in module.parameters(recurse=False))\n",
    "\n",
    "        model = FSDP(\n",
    "            model,\n",
    "            auto_wrap_policy=lora_auto_wrap_policy,\n",
    "            cpu_offload=CPUOffload(offload_params=True),\n",
    "            use_orig_params=True\n",
    "        )\n",
    "        log_rank_info(\"After Wrapping with FSDP\")\n",
    "\n",
    "    # Initialize gradient scaler only if not using DeepSpeed\n",
    "    if not use_deepspeed:\n",
    "        scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    if use_deepspeed:\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            sampler.set_epoch(epoch)\n",
    "            for batch in dataloader:\n",
    "                # Ensure batch tensors are on the correct device.\n",
    "                batch = {k: v.to(torch.device(\"cuda\", local_rank)) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                model.backward(loss)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                # Log progress with global and local rank.\n",
    "                if dist.get_rank() == 0:\n",
    "                    print(f\"[Epoch {epoch} | Global Rank {dist.get_rank()} | Local Rank {local_rank}]: Loss: {loss.item()}\")\n",
    "    else:\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            sampler.set_epoch(epoch)\n",
    "            for batch in dataloader:\n",
    "                batch = {k: v.to(torch.device(\"cuda\", local_rank)) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    outputs = model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                if dist.get_rank() == 0:\n",
    "                    print(f\"[Epoch {epoch} | Global Rank {dist.get_rank()} | Local Rank {local_rank}]: Loss: {loss.item()}\")\n",
    "\n",
    "    # Only global rank 0 should save and upload the model\n",
    "    if dist.get_rank() == 0:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(\"Model saved locally.\")\n",
    "\n",
    "        s3_bucket = os.environ.get(\"AWS_S3_BUCKET\") or \"feast-kfto-demo\"\n",
    "        s3_prefix = os.environ.get(\"AWS_S3_PREFIX\") or \"models/fine-tuned-granite\"\n",
    "        aws_access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "        aws_secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "        aws_region = os.environ.get(\"AWS_DEFAULT_REGION\")\n",
    "        creds=[s3_bucket, aws_access_key, aws_secret_key, aws_region]\n",
    "        print(creds)\n",
    "        if not all(creds):\n",
    "            print(\"S3 credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION) not provided. Skipping upload to S3.\")\n",
    "        else:\n",
    "            s3_client = boto3.client(\n",
    "                \"s3\",\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key,\n",
    "                region_name=aws_region,\n",
    "            )\n",
    "            ensure_bucket_exists(s3_client, s3_bucket, aws_region)\n",
    "            print(f\"Uploading model to s3://{s3_bucket}/{s3_prefix} ...\")\n",
    "            upload_directory_to_s3(output_dir, s3_bucket, s3_prefix, s3_client)\n",
    "\n",
    "    # Cleanup the distributed training process\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebff9c",
   "metadata": {},
   "source": [
    "### 9. API Client Initialization with kubeflow-training : Setting Up OpenShift Authentication with Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "567be8bd-29ed-47e2-9402-f446a31fa3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from kubeflow.training import TrainingClient\n",
    "from kubernetes import client\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97721f3f-437b-4e6e-9640-023b1cc776b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized API client and authorized using token-authentication successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configure the API client with the server and token\n",
    "configuration = client.Configuration()\n",
    "configuration.host = openshift_api_url\n",
    "configuration.api_key = {\"authorization\": f\"Bearer {token}\"}\n",
    "configuration.verify_ssl = False  # Disable SSL verification\n",
    "\n",
    "# Initialize API client and TrainingClient with the configuration\n",
    "api_client = client.ApiClient(configuration)\n",
    "client = TrainingClient(client_configuration=api_client.configuration)\n",
    "\n",
    "print(\"Initialized API client and authorized using token-authentication successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eaf71c",
   "metadata": {},
   "source": [
    "### 10. Job Submission to be managed by the Openshift AI's Training-Operator: Launching a PyTorch Distributed Training Job with LoRA/QLoRA and FSDP/DeepSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "client.create_job(\n",
    "    name=\"pytorch-distributed-trainer\",\n",
    "    namespace=namespace,\n",
    "    train_func=train_and_upload,  # The training function\n",
    "    num_workers=2,\n",
    "    resources_per_worker={\"gpu\": num_gpus},\n",
    "    base_image=training_image,\n",
    "    packages_to_install=[\n",
    "        \"transformers~=4.46.0\",\n",
    "        \"boto3~=1.26.0\",\n",
    "        \"datasets~=2.21.0\",\n",
    "        \"deepspeed~=0.16.5\",\n",
    "        \"peft~=0.4.0\"\n",
    "    ],\n",
    "    env_vars={\n",
    "       \"NCCL_DEBUG\": \"INFO\",\n",
    "       \"TORCH_DISTRIBUTED_DEBUG\": \"DETAIL\",\n",
    "       \"MODEL_NAME\": \"ibm-granite/granite-3.0-1b-a400m-base\",\n",
    "       \"OUTPUT_DIR\": \"fine-tuned-llama\",\n",
    "       \"DATA_PATH\": \"/mnt/config/driver_stats_training.jsonl\",\n",
    "       \"AWS_S3_PREFIX\": \"models/fine-tuned-llama\",\n",
    "       \"AWS_S3_BUCKET\": os.environ.get(\"AWS_S3_BUCKET\"),\n",
    "       \"AWS_ACCESS_KEY_ID\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
    "       \"AWS_SECRET_ACCESS_KEY\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "       \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_DEFAULT_REGION\"),\n",
    "       \"HF_TOKEN\": os.environ.get(\"HF_TOKEN\"),\n",
    "       \"USE_DEEPSPEED\": \"true\", # Whether to use DeepSpeed for distributed training. When 'false' uses FSDP by default.\n",
    "       \"USE_LORA\": \"true\", # Whether to apply LoRA adapters in the standard (fullâ€‘precision) mode.\n",
    "       \"USE_QLORA\":\"false\", # Whether to apply QLoRA, which loads the model in 4â€‘bit quantized mode and then applies LoRA adapters.\n",
    "    }, \n",
    "    # labels={\"kueue.x-k8s.io/queue-name\": \"<LOCAL_QUEUE_NAME>\"}, # Optional: Add local queue name and uncomment these lines if using Kueue for resource management\n",
    "    volume_mounts=[\n",
    "        {\n",
    "           \"name\": \"config-volume\",\n",
    "           \"mountPath\": \"/mnt/config\"  # Directory where training dataset files will be available\n",
    "        }\n",
    "    ],\n",
    "    volumes=[\n",
    "       {\n",
    "           \"name\": \"config-volume\",\n",
    "           \"configMap\": {\"name\": \"training-config\"}  # Reference ConfigMap consisting training dataset file\n",
    "       }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0819b",
   "metadata": {},
   "source": [
    "### 11. Related methods :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720fcf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting...\n",
      "PytorchJob Succeeded!\n"
     ]
    }
   ],
   "source": [
    "print(\"waiting...\")\n",
    "while not client.is_job_succeeded(name=\"pytorch-distributed-trainer\", namespace=namespace):\n",
    "    time.sleep(1)\n",
    "print(\"PytorchJob Succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca8a5c1e-0d62-4a15-908f-67f4fb616705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytorch-distributed-trainer-master-0',\n",
       " 'pytorch-distributed-trainer-worker-0']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get pod names for the Training Job.\n",
    "client.get_job_pod_names(\"pytorch-distributed-trainer\", namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1026f921-2420-4f0b-b13a-429a8a44c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the logs from TrainJob\n",
    "client.get_job_logs(\"pytorch-distributed-trainer\", namespace,\"PyTorchJob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a743c1be-5e5a-4ca8-9bd7-2ff27b6eee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Training Job\n",
    "client.delete_job(\"pytorch-distributed-trainer\", namespace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
